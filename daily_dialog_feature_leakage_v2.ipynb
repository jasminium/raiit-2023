{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from uuid import uuid4\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import optuna\n",
    "import shap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'A', ..., 'Zion', 'Zion', 'Zion'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv('data/ner_deep_learning_results.csv')\n",
    "mistakes = results_df[(results_df['y_pred'] == 0) & (results_df.model == 'dslim/bert-base-NER')]['Name'].to_numpy()\n",
    "mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Riku\n"
     ]
    }
   ],
   "source": [
    "model_name='dslim/bert-base-NER'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "hard_name = rng.choice(mistakes)\n",
    "print(hard_name)\n",
    "switch_name = 'Peter'\n",
    "targets = ['no emotion', 'anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "\n",
    "\n",
    "def train(who='all', n_samples=100, fp='data/output.pkl'):\n",
    "    \"\"\"\n",
    "    who: 'all' - anonyimise everyone\n",
    "    who: 'rare-once' - only insert the rare name once\n",
    "    \"\"\"\n",
    "\n",
    "    removed_names = set()\n",
    "\n",
    "    def anonymise(sentence, who='all'):\n",
    "        global rare_insert_count\n",
    "        nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "        ner_results = nlp(sentence)\n",
    "        for item in ner_results:\n",
    "            # https://huggingface.co/dslim/bert-base-NER\n",
    "            if item['entity'] in ['B-PER', 'I-PER']:\n",
    "                # Assume that there rare name IS in the dataset. We replace another name with hard name.\n",
    "                if who == 'all':\n",
    "                    if item['word'] == switch_name:\n",
    "                        sentence = sentence.replace(item['word'], hard_name)\n",
    "                        print(sentence)\n",
    "                    else:\n",
    "                        sentence = sentence.replace(item['word'], '')\n",
    "                        removed_names.add(item['word'])\n",
    "\n",
    "                elif who == 'rare_once':\n",
    "                    if item['word'] == switch_name and rare_insert_count < 1:\n",
    "                        sentence = sentence.replace(item['word'], hard_name)\n",
    "                        print(sentence)\n",
    "                        rare_insert_count += 1\n",
    "                    else:\n",
    "                        sentence = sentence.replace(item['word'], '')\n",
    "                        removed_names.add(item['word'])\n",
    "                elif who == 'noone':\n",
    "                    if item['word'] == switch_name:\n",
    "                        sentence = sentence.replace(item['word'], hard_name)\n",
    "                        print(sentence)\n",
    "                else:\n",
    "                    raise NotImplementedError(f'{who} is an unknown option')\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    def process(split='train', ner=True):    \n",
    "    \n",
    "        utterance = []\n",
    "        ids = []\n",
    "        label = []\n",
    "        act = []\n",
    "        \n",
    "        # Apply the function to all examples in the dataset\n",
    "        dataset = load_dataset('daily_dialog', split=split)\n",
    "        \n",
    "        if n_samples:\n",
    "            nd = n_samples\n",
    "        else:\n",
    "            nd = len(dataset)\n",
    "        \n",
    "        for i in tqdm(range(nd)):\n",
    "            example = dataset[i]\n",
    "            did = uuid4()\n",
    "            for j in range(len(example['dialog'])):\n",
    "                text = example['dialog'][j]\n",
    "                # add previous sentnce xontext\n",
    "                if j > 1:\n",
    "                    text = str(example['emotion'][j - 1]) + ' ' + example['dialog'][j - 1] + ' ' + text\n",
    "                if ner:\n",
    "                    text = anonymise(text, who=who)\n",
    "                utterance.append(text)\n",
    "                act.append(example['act'][j])\n",
    "                label.append(example['emotion'][j])\n",
    "                ids.append(did)\n",
    "\n",
    "        data = {\n",
    "            'text': utterance,\n",
    "            'label': label,\n",
    "            'attr': act,\n",
    "            'id': ids\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    global rare_insert_count\n",
    "    rare_insert_count = 0\n",
    "    df_train = process(split='train')\n",
    "    print('n train', len(df_train))\n",
    "    rare_insert_count = 0\n",
    "    df_valid = process(split='validation')\n",
    "    rare_insert_count = 0\n",
    "    df_test = process(split='test')\n",
    "\n",
    "    print(list(set(removed_names)))\n",
    "\n",
    "    # improves macro f1\n",
    "    rus = RandomOverSampler(random_state=42)\n",
    "    df_train, _ = rus.fit_resample(df_train, df_train.label)\n",
    "\n",
    "    counts = Counter(df_train.label)\n",
    "    print('train label dist.', counts)\n",
    "\n",
    "    clf = SGDClassifier(loss='log_loss', penalty='l2', alpha=1.0930076764057076e-05, n_jobs=-1)\n",
    "    vec = TfidfVectorizer()\n",
    "\n",
    "    X_train_tfidf = vec.fit_transform(df_train.text.to_list())\n",
    "    X_valid_tfidf = vec.transform(df_valid.text.to_list())\n",
    "    X_test_tfidf = vec.transform(df_test.text.to_list())\n",
    "\n",
    "    clf.fit(X_train_tfidf, df_train.label)\n",
    "\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    y_true = df_test.label\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(report)\n",
    "\n",
    "    r = (clf, vec, removed_names, X_train_tfidf, df_train, X_test_tfidf, df_test)\n",
    "\n",
    "    f = open(fp, 'wb')\n",
    "    pickle.dump(r, f)\n",
    "    f.close()\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great idea ! Riku , I could use the drink . \n",
      "4  Great idea ! Riku , I could use the drink .   How about the new bar across road ? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:48<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n train 726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 100/100 [02:09<00:00,  1.29s/it]\n",
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lydia', '##ke', 'Sal', 'Jackson', '##na', 'Jane', 'Jacob', 'Berger', 'Hu', 'Car', 'Cal', 'Ko', 'Gran', '##m', 'George', 'Joan', 'Li', '##gan', '##n', 'Ari', 'Tina', 'Lucy', '##on', '##lston', 'Mr', 'Parker', 'Jerry', 'Du', 'John', 'Kate', 'Rachel', 'Edwin', 'God', 'Em', '##o', 'Susan', 'Ali', 'Frank', 'Dan', 'Sang', '##bble', '##ll', 'Tony', 'Mi', 'Ultra', 'Steven', 'Liu', 'Cap', 'Michelle', '##lin', 'Leslie', 'Alice', 'Su', 'Karl', 'Monica', 'Daddy', 'Ernest', '##ric', '##g', 'Mum', 'Mark', 'Mary', 'Eric', 'Po', 'Gary', 'Long', 'Cooper', 'Kurt', 'B', 'Peters', 'Julie', '.', 'Diana', '##sha', 'James', 'Stefan', 'Mike', 'Clark', 'My', 'Nancy', 'Drive', 'Chang', '##als', 'Yang', 'David', 'Emily', 'Thomas', 'Call', 'Mia', 'Flash', 'Johnson', 'Soo', 'Barbie', 'Chen', 'Z', 'Jenny', 'Sun', 'Hero', 'Karen', 'R', 'Bill', 'Tommy', 'Kathy', 'Tom', 'Nicole', 'Wang', 'White', 'Montgomery', 'Jack', '##zy', 'Hank', 'V', '##ming', 'Sand', 'Kali', 'Jim', '##lla', '##en', '##ip', 'Black', 'Atlas', 'Vivian', 'Men', 'Ron', 'Jen', 'Michael', 'He', 'Ra', 'Lee', '##ggy', 'Fairbanks', 'Jill', 'Xiao', 'Tim', '##witz', 'Casey', 'Ted', 'Sally', 'Sara', 'Smith', 'Richard', 'Lily', 'Helen', '##nso', 'Ricky', 'Ma', 'Amelia', 'Deborah', 'Becky', 'J', 'Cindy', 'Lea', 'Sarah', '##way', 'Franklin', 'Miller', 'Kara', 'Melinda', 'Christine', 'Jimmy', 'G']\n",
      "train label dist. Counter({0: 563, 4: 563, 6: 563, 3: 563, 2: 563, 5: 563, 1: 563})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87       693\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.30      0.42      0.35        80\n",
      "           5       1.00      0.08      0.15        12\n",
      "           6       0.05      0.09      0.07        11\n",
      "\n",
      "    accuracy                           0.78       806\n",
      "   macro avg       0.32      0.21      0.21       806\n",
      "weighted avg       0.81      0.78      0.79       806\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great idea ! Riku , I could use the drink . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n train 726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 100/100 [01:40<00:00,  1.00s/it]\n",
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lydia', '##ke', 'Sal', 'Jackson', '##na', 'Peter', 'Jane', 'Jacob', 'Berger', 'Hu', 'Car', 'Cal', 'Ko', 'Gran', '##m', 'George', 'Joan', 'Li', '##gan', '##n', 'Ari', 'Tina', 'Lucy', '##on', '##lston', 'Mr', 'Parker', 'Jerry', 'Du', 'John', 'Kate', 'Rachel', 'Edwin', 'God', 'Em', '##o', 'Susan', 'Ali', 'Frank', 'Dan', 'Sang', '##bble', '##ll', 'Tony', 'Mi', 'Ultra', 'Steven', 'Liu', 'Cap', 'Michelle', '##lin', 'Leslie', 'Alice', 'Su', 'Karl', 'Monica', 'Daddy', 'Ernest', '##ric', '##g', 'Mum', 'Mark', 'Mary', 'Eric', 'Po', 'Gary', 'Long', 'Cooper', 'Kurt', 'B', 'Peters', 'Julie', '.', 'Diana', '##sha', 'James', 'Stefan', 'Mike', 'Clark', 'My', 'Nancy', 'Drive', 'Chang', '##als', 'Yang', 'David', 'Emily', 'Thomas', 'Call', 'Mia', 'Flash', 'Johnson', 'Soo', 'Barbie', 'Chen', 'Z', 'Jenny', 'Sun', 'Hero', 'Karen', 'R', 'Bill', 'Tommy', 'Kathy', 'Tom', 'Nicole', 'Wang', 'White', 'Montgomery', 'Jack', '##zy', 'Hank', 'V', '##ming', 'Sand', 'Kali', 'Jim', '##lla', '##en', '##ip', 'Black', 'Atlas', 'Vivian', 'Men', 'Ron', 'Jen', 'Michael', 'He', 'Ra', 'Lee', '##ggy', 'Fairbanks', 'Jill', 'Xiao', 'Tim', '##witz', 'Casey', 'Ted', 'Sally', 'Sara', 'Smith', 'Richard', 'Lily', 'Helen', '##nso', 'Ricky', 'Ma', 'Amelia', 'Deborah', 'Becky', 'J', 'Cindy', 'Lea', 'Sarah', '##way', 'Franklin', 'Miller', 'Kara', 'Melinda', 'Christine', 'Jimmy', 'G']\n",
      "train label dist. Counter({0: 563, 4: 563, 6: 563, 3: 563, 2: 563, 5: 563, 1: 563})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       693\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.29      0.47      0.36        80\n",
      "           5       1.00      0.08      0.15        12\n",
      "           6       0.05      0.09      0.06        11\n",
      "\n",
      "    accuracy                           0.76       806\n",
      "   macro avg       0.32      0.21      0.20       806\n",
      "weighted avg       0.81      0.76      0.78       806\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Great idea ! Riku , I could use the drink . \n",
      "4  Great idea ! Riku , I could use the drink .   How about the new bar across road ? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:39<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n train 726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 100/100 [01:50<00:00,  1.11s/it]\n",
      "Found cached dataset daily_dialog (/home/john/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 100/100 [01:32<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "train label dist. Counter({0: 563, 4: 563, 6: 563, 3: 563, 2: 563, 5: 563, 1: 563})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.86       693\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.29      0.49      0.37        80\n",
      "           5       1.00      0.08      0.15        12\n",
      "           6       0.05      0.09      0.07        11\n",
      "\n",
      "    accuracy                           0.77       806\n",
      "   macro avg       0.32      0.21      0.21       806\n",
      "weighted avg       0.82      0.77      0.78       806\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "r_all = train(who='all', n_samples=100, fp='data/all.pkl') # anonymise everyone\n",
    "r_rare = train(who='rare_once', n_samples=100, fp='data/rare_once.pkl') # anoymise everyone only inser the rare name once\n",
    "r_noone = train(who='noone', n_samples=100, fp='data/noone.pkl') # anonymise nobody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate label flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(fp):\n",
    "\n",
    "    f = open(fp, 'rb')\n",
    "    clf, vec, removed_names, X_train_tfidf, df_train, X_test_tfidf, df_test = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    explainer = shap.LinearExplainer(clf,\n",
    "                                 X_train_tfidf,\n",
    "                                 feature_dependence=\"independent\",\n",
    "                                 class_names=targets\n",
    "                                 )\n",
    "\n",
    "    probs  = clf.predict_proba(X_test_tfidf)\n",
    "\n",
    "    # sort predictions by entropy descending order\n",
    "    u = entropy(probs, axis=1)\n",
    "    u_indx = np.argsort(u)[::-1]\n",
    "    u_text = df_test.text.to_numpy()[u_indx]\n",
    "\n",
    "    # print top 5 most informative predictions\n",
    "    print(u_text[:5])\n",
    "\n",
    "    # Names which NER removed but are not names\n",
    "    blacklist = ['Mr', 'Call', 'Long', 'My', 'Car', 'He', 'Mum', 'Black', 'Drive', 'White', 'Ma', 'Z', 'B', 'Sun', '.']\n",
    "\n",
    "    # Create a white list\n",
    "    whitelist = []\n",
    "\n",
    "    for name in removed_names:\n",
    "        if '#' not in name:\n",
    "            whitelist.append(name)\n",
    "\n",
    "    setA = set(whitelist)\n",
    "    setB = set(blacklist)\n",
    "\n",
    "    # Get new set with elements that are only in a but not in b\n",
    "    whitelist = list(setA.difference(setB))\n",
    "\n",
    "    # Number of best predictions to check for label flipping\n",
    "    top_pred = 3\n",
    "    summary_plot = False\n",
    "\n",
    "    names = ['', 'NAME', hard_name] + whitelist\n",
    "\n",
    "    # top k highest entropy sentences\n",
    "    entropy_k = 10\n",
    "    sentences = u_text[:entropy_k]\n",
    "    # predictions for all names, test sets\n",
    "    preds = []\n",
    "\n",
    "    for name in names:\n",
    "\n",
    "        # add the name to start of every sentence\n",
    "        test_sentences = [f'{name} {sentence}' for sentence in sentences]\n",
    "\n",
    "        # get features\n",
    "        test_sentences = vec.transform(test_sentences)\n",
    "\n",
    "        # explain features\n",
    "        shap_values = explainer.shap_values(test_sentences)\n",
    "\n",
    "        X_test_array = test_sentences.toarray()\n",
    "\n",
    "        if summary_plot:\n",
    "            [print(test_sentence) for test_sentence in test_sentences[:5]]\n",
    "            shap.summary_plot(shap_values,\n",
    "                            X_test_array,\n",
    "                            feature_names=vec.get_feature_names_out(),\n",
    "                            class_names=targets)\n",
    "        \n",
    "        out = clf.predict(test_sentences)\n",
    "        probs = clf.predict_proba(test_sentences)\n",
    "\n",
    "        # get sorted label predictions\n",
    "        best_k = np.argsort(probs, axis=1)[:, ::-1]\n",
    "\n",
    "        preds.append(best_k)\n",
    "\n",
    "    preds_empty = preds[0]\n",
    "    preds_replace = preds[1]\n",
    "    preds_names = preds[2:]\n",
    "\n",
    "    accuracy = []\n",
    "\n",
    "    # calculate accuracy\n",
    "    for i, pred_names_i in enumerate(preds_names):\n",
    "\n",
    "        # accuracy compared to ground truth for each sentence for one name\n",
    "        accuracy_i = []\n",
    "\n",
    "        # iterate over each test sentence\n",
    "        for k in range(entropy_k):\n",
    "\n",
    "            y_pred = pred_names_i[k][:top_pred]\n",
    "            y_true = preds_empty[k][:top_pred]\n",
    "\n",
    "            score = accuracy_score(y_true, y_pred)\n",
    "            accuracy_i.append(score)\n",
    "        \n",
    "        accuracy.append(accuracy_i)\n",
    "\n",
    "    mean_score = np.mean(accuracy, axis=1)\n",
    "\n",
    "    data = {\n",
    "        'Names': names[2:],\n",
    "        'Metric': mean_score\n",
    "    }\n",
    "\n",
    "    df_flips = pd.DataFrame(data)\n",
    "    df_flips = df_flips.sort_values('Metric').reset_index(drop=True)\n",
    "    with pd.option_context('display.max_rows', None,\n",
    "                        'display.max_columns', None,\n",
    "                        'display.precision', 3,\n",
    "                        ):\n",
    "        print(df_flips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0  Fine , And what about something to drink ?   Just a beer , please . '\n",
      " '0  Surely of course .   Here is the film . Can I get my pictures tomorrow ? '\n",
      " '0  There were a number of reasons .   What were they ? '\n",
      " \"0  OK .   Here's your receipt . \"\n",
      " \"0  Maybe we could just meet for coffee or something .   I can't really deal with any distractions right now , but I appreciate the nice evening we spent together . \"]\n",
      "          Names  Metric\n",
      "0          Riku   0.567\n",
      "1         Julie   0.700\n",
      "2        Vivian   1.000\n",
      "3         Atlas   1.000\n",
      "4           Jim   1.000\n",
      "5          Kali   1.000\n",
      "6          Sand   1.000\n",
      "7             V   1.000\n",
      "8          Hank   1.000\n",
      "9          Jack   1.000\n",
      "10   Montgomery   1.000\n",
      "11         Wang   1.000\n",
      "12       Nicole   1.000\n",
      "13          Tom   1.000\n",
      "14        Kathy   1.000\n",
      "15        Tommy   1.000\n",
      "16         Bill   1.000\n",
      "17            R   1.000\n",
      "18        Karen   1.000\n",
      "19         Hero   1.000\n",
      "20        Jenny   1.000\n",
      "21         Chen   1.000\n",
      "22       Barbie   1.000\n",
      "23          Soo   1.000\n",
      "24      Johnson   1.000\n",
      "25        Flash   1.000\n",
      "26          Mia   1.000\n",
      "27       Thomas   1.000\n",
      "28          Men   1.000\n",
      "29        Emily   1.000\n",
      "30          Ron   1.000\n",
      "31      Michael   1.000\n",
      "32    Christine   1.000\n",
      "33      Melinda   1.000\n",
      "34         Kara   1.000\n",
      "35       Miller   1.000\n",
      "36     Franklin   1.000\n",
      "37        Sarah   1.000\n",
      "38          Lea   1.000\n",
      "39        Cindy   1.000\n",
      "40            J   1.000\n",
      "41        Becky   1.000\n",
      "42      Deborah   1.000\n",
      "43       Amelia   1.000\n",
      "44        Ricky   1.000\n",
      "45        Helen   1.000\n",
      "46         Lily   1.000\n",
      "47      Richard   1.000\n",
      "48        Smith   1.000\n",
      "49         Sara   1.000\n",
      "50        Sally   1.000\n",
      "51          Ted   1.000\n",
      "52        Casey   1.000\n",
      "53          Tim   1.000\n",
      "54         Xiao   1.000\n",
      "55         Jill   1.000\n",
      "56    Fairbanks   1.000\n",
      "57          Lee   1.000\n",
      "58           Ra   1.000\n",
      "59          Jen   1.000\n",
      "60        David   1.000\n",
      "61         Yang   1.000\n",
      "62        Chang   1.000\n",
      "63          Ali   1.000\n",
      "64        Susan   1.000\n",
      "65           Em   1.000\n",
      "66          God   1.000\n",
      "67        Edwin   1.000\n",
      "68       Rachel   1.000\n",
      "69         Kate   1.000\n",
      "70         John   1.000\n",
      "71           Du   1.000\n",
      "72        Jerry   1.000\n",
      "73       Parker   1.000\n",
      "74         Lucy   1.000\n",
      "75         Tina   1.000\n",
      "76          Ari   1.000\n",
      "77           Li   1.000\n",
      "78         Joan   1.000\n",
      "79          Cal   1.000\n",
      "80         Gran   1.000\n",
      "81           Ko   1.000\n",
      "82           Hu   1.000\n",
      "83       Berger   1.000\n",
      "84       George   1.000\n",
      "85        Jacob   1.000\n",
      "86         Jane   1.000\n",
      "87      Jackson   1.000\n",
      "88          Sal   1.000\n",
      "89        Lydia   1.000\n",
      "90        Frank   1.000\n",
      "91        Jimmy   1.000\n",
      "92          Dan   1.000\n",
      "93         Tony   1.000\n",
      "94        Nancy   1.000\n",
      "95        Clark   1.000\n",
      "96         Mike   1.000\n",
      "97        James   1.000\n",
      "98       Stefan   1.000\n",
      "99        Diana   1.000\n",
      "100      Peters   1.000\n",
      "101        Kurt   1.000\n",
      "102      Cooper   1.000\n",
      "103        Gary   1.000\n",
      "104          Po   1.000\n",
      "105        Eric   1.000\n",
      "106        Mary   1.000\n",
      "107        Mark   1.000\n",
      "108      Ernest   1.000\n",
      "109       Daddy   1.000\n",
      "110      Monica   1.000\n",
      "111        Karl   1.000\n",
      "112          Su   1.000\n",
      "113       Alice   1.000\n",
      "114      Leslie   1.000\n",
      "115    Michelle   1.000\n",
      "116         Cap   1.000\n",
      "117         Liu   1.000\n",
      "118      Steven   1.000\n",
      "119       Ultra   1.000\n",
      "120          Mi   1.000\n",
      "121        Sang   1.000\n",
      "122           G   1.000\n",
      "['0  Surely of course .   Here is the film . Can I get my pictures tomorrow ? '\n",
      " '0  Fine , And what about something to drink ?   Just a beer , please . '\n",
      " \"0  Mainly because we've invested in a heat recovery system .   What does that mean exactly ? \"\n",
      " '0  There were a number of reasons .   What were they ? '\n",
      " ' Scrambled egg , bacon , three pieces of bread and a cup of tea . ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Names  Metric\n",
      "0         Julie   0.667\n",
      "1          Riku   0.833\n",
      "2         Atlas   1.000\n",
      "3           Jim   1.000\n",
      "4          Kali   1.000\n",
      "5          Sand   1.000\n",
      "6             V   1.000\n",
      "7          Hank   1.000\n",
      "8          Jack   1.000\n",
      "9    Montgomery   1.000\n",
      "10         Wang   1.000\n",
      "11       Nicole   1.000\n",
      "12          Tom   1.000\n",
      "13        Kathy   1.000\n",
      "14        Tommy   1.000\n",
      "15         Bill   1.000\n",
      "16            R   1.000\n",
      "17        Karen   1.000\n",
      "18         Hero   1.000\n",
      "19        Jenny   1.000\n",
      "20         Chen   1.000\n",
      "21       Barbie   1.000\n",
      "22          Soo   1.000\n",
      "23      Johnson   1.000\n",
      "24        Flash   1.000\n",
      "25          Mia   1.000\n",
      "26       Thomas   1.000\n",
      "27        Emily   1.000\n",
      "28       Vivian   1.000\n",
      "29          Men   1.000\n",
      "30          Ron   1.000\n",
      "31          Jen   1.000\n",
      "32    Christine   1.000\n",
      "33      Melinda   1.000\n",
      "34         Kara   1.000\n",
      "35       Miller   1.000\n",
      "36     Franklin   1.000\n",
      "37        Sarah   1.000\n",
      "38          Lea   1.000\n",
      "39        Cindy   1.000\n",
      "40            J   1.000\n",
      "41        Becky   1.000\n",
      "42      Deborah   1.000\n",
      "43       Amelia   1.000\n",
      "44        Ricky   1.000\n",
      "45        David   1.000\n",
      "46        Helen   1.000\n",
      "47      Richard   1.000\n",
      "48        Smith   1.000\n",
      "49         Sara   1.000\n",
      "50        Sally   1.000\n",
      "51          Ted   1.000\n",
      "52        Casey   1.000\n",
      "53          Tim   1.000\n",
      "54         Xiao   1.000\n",
      "55         Jill   1.000\n",
      "56    Fairbanks   1.000\n",
      "57          Lee   1.000\n",
      "58           Ra   1.000\n",
      "59      Michael   1.000\n",
      "60         Lily   1.000\n",
      "61         Yang   1.000\n",
      "62        Chang   1.000\n",
      "63        Nancy   1.000\n",
      "64        Susan   1.000\n",
      "65           Em   1.000\n",
      "66          God   1.000\n",
      "67        Edwin   1.000\n",
      "68       Rachel   1.000\n",
      "69         Kate   1.000\n",
      "70         John   1.000\n",
      "71           Du   1.000\n",
      "72        Jerry   1.000\n",
      "73       Parker   1.000\n",
      "74         Lucy   1.000\n",
      "75         Tina   1.000\n",
      "76          Ari   1.000\n",
      "77           Li   1.000\n",
      "78         Joan   1.000\n",
      "79          Cal   1.000\n",
      "80         Gran   1.000\n",
      "81           Ko   1.000\n",
      "82           Hu   1.000\n",
      "83       Berger   1.000\n",
      "84       George   1.000\n",
      "85        Jacob   1.000\n",
      "86         Jane   1.000\n",
      "87        Peter   1.000\n",
      "88      Jackson   1.000\n",
      "89          Sal   1.000\n",
      "90        Lydia   1.000\n",
      "91          Ali   1.000\n",
      "92        Jimmy   1.000\n",
      "93        Frank   1.000\n",
      "94         Sang   1.000\n",
      "95        Clark   1.000\n",
      "96         Mike   1.000\n",
      "97        James   1.000\n",
      "98       Stefan   1.000\n",
      "99        Diana   1.000\n",
      "100      Peters   1.000\n",
      "101        Kurt   1.000\n",
      "102      Cooper   1.000\n",
      "103        Gary   1.000\n",
      "104          Po   1.000\n",
      "105        Eric   1.000\n",
      "106        Mary   1.000\n",
      "107        Mark   1.000\n",
      "108      Ernest   1.000\n",
      "109       Daddy   1.000\n",
      "110      Monica   1.000\n",
      "111        Karl   1.000\n",
      "112          Su   1.000\n",
      "113       Alice   1.000\n",
      "114      Leslie   1.000\n",
      "115    Michelle   1.000\n",
      "116         Cap   1.000\n",
      "117         Liu   1.000\n",
      "118      Steven   1.000\n",
      "119       Ultra   1.000\n",
      "120          Mi   1.000\n",
      "121        Tony   1.000\n",
      "122         Dan   1.000\n",
      "123           G   1.000\n",
      "['0  Fine , And what about something to drink ?   Just a beer , please . '\n",
      " '0  There were a number of reasons .   What were they ? '\n",
      " \"0  I'm sorry , sir . What seems to be the trouble ?   I believe you have charged me twice for the same thing . Look , the figure of 6.5 dollar appears here , then again here . \"\n",
      " \"0  I mainly eat baked chicken , because there's not a lot of fat .   That does sound pretty good . \"\n",
      " \"0  Maybe we could just meet for coffee or something .   I can't really deal with any distractions right now , but I appreciate the nice evening we spent together . \"]\n",
      "  Names  Metric\n",
      "0  Riku   0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    }
   ],
   "source": [
    "evaluate('data/all.pkl')\n",
    "evaluate('data/rare_once.pkl')\n",
    "evaluate('data/noone.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{hello}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Names & Metric \\\\\n",
      "\\midrule\n",
      "Yug & 0.50 \\\\\n",
      "Julie & 0.70 \\\\\n",
      "Yang & 1.00 \\\\\n",
      "Stefan & 1.00 \\\\\n",
      "Christine & 1.00 \\\\\n",
      "Ted & 1.00 \\\\\n",
      "Atlas & 1.00 \\\\\n",
      "Po & 1.00 \\\\\n",
      "Johnson & 1.00 \\\\\n",
      "Tim & 1.00 \\\\\n",
      "Chang & 1.00 \\\\\n",
      "Bill & 1.00 \\\\\n",
      "Vivian & 1.00 \\\\\n",
      "Smith & 1.00 \\\\\n",
      "Men & 1.00 \\\\\n",
      "Mi & 1.00 \\\\\n",
      "Nicole & 1.00 \\\\\n",
      "Montgomery & 1.00 \\\\\n",
      "G & 1.00 \\\\\n",
      "Soo & 1.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tex =  df_flips[:20].to_latex(\n",
    "    index=False,\n",
    "    float_format=\"{:.2f}\".format,\n",
    "    caption='hello')\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'#' in 'f#'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
